{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6e1489",
   "metadata": {},
   "source": [
    "# Quantum-Classical Hybrid Autoencoder for Medical Image Classification\n",
    "\n",
    "This notebook implements a hybrid quantum-classical model for chest X-ray classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "## Installation (Run if packages missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install packages\n",
    "# %pip install torch torchvision numpy pennylane medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89333376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.8.0\n",
      "PennyLane version: 0.42.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PennyLane version: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a80afb",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c01a2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Qubits: 6\n",
      "  Quantum layers: 2\n",
      "  Latent dimension: 64\n",
      "  Image size: 224x224\n",
      "  Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 6                 # set desired number of qubits\n",
    "n_layers = 2                 # set desired number of quantum layers\n",
    "latent_dim = 2**n_qubits     # latent dimension = 2^n_qubits\n",
    "img_size = 224               # set desired image size (28, 64, 128, 224)\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Qubits: {n_qubits}\")\n",
    "print(f\"  Quantum layers: {n_layers}\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Image size: {img_size}x{img_size}\")\n",
    "print(f\"  Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74359e0",
   "metadata": {},
   "source": [
    "## Load Data from preprocess.py\n",
    "\n",
    "We import the data loaders from our preprocess.py file to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24e8e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training samples: 78468\n",
      "Test samples: 22433\n",
      "Training batches: 2452\n",
      "Test batches: 702\n"
     ]
    }
   ],
   "source": [
    "# Import data loaders from preprocess.py\n",
    "from preprocess import train_loader, test_loader, train_dataset, test_dataset\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd582d",
   "metadata": {},
   "source": [
    "Filter Out Double-Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "14fc1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for single-label images only...\n",
      "✓ Filtered Training: 21602 single-label images (675 batches)\n",
      "✓ Filtered Test: 6259 single-label images (196 batches)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "print(\"Filtering for single-label images only...\")\n",
    "\n",
    "# Filter training set\n",
    "train_single_label_indices = []\n",
    "for i in range(len(train_dataset)):\n",
    "    _, label = train_dataset[i]\n",
    "    if label.sum() == 1:  # Only one condition present\n",
    "        train_single_label_indices.append(i)\n",
    "\n",
    "train_dataset = Subset(train_dataset, train_single_label_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Filter test set\n",
    "test_single_label_indices = []\n",
    "for i in range(len(test_dataset)):\n",
    "    _, label = test_dataset[i]\n",
    "    if label.sum() == 1:\n",
    "        test_single_label_indices.append(i)\n",
    "\n",
    "test_dataset = Subset(test_dataset, test_single_label_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"✓ Filtered Training: {len(train_dataset)} single-label images ({len(train_loader)} batches)\")\n",
    "print(f\"✓ Filtered Test: {len(test_dataset)} single-label images ({len(test_loader)} batches)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf487ff",
   "metadata": {},
   "source": [
    "## Classical Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07cc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64, img_size=224):\n",
    "        super().__init__()\n",
    "        self.h8 = img_size // 8\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize encoder weights properly\n",
    "        for m in self.encoder.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * self.h8 * self.h8),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, self.h8, self.h8)),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # (B,latent_dim)\n",
    "        # REMOVED: z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        x_rec = self.decoder(z)\n",
    "        return z, x_rec  # Return unnormalized z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4a989",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6e4fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum device initialized: <default.qubit device (wires=6) at 0x3010d4250>\n",
      "QuantumHead with EXTREME safety checks!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- quantum device ----\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "def qnode(inputs, weights):\n",
    "    # inputs: shape (2**n_qubits,), NOT batched - single sample\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    def __init__(self, n_layers, n_qubits, n_classes, latent_dim):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.encoder_fc = nn.Linear(latent_dim, 2**n_qubits)\n",
    "        \n",
    "        # Better weight initialization\n",
    "        nn.init.xavier_uniform_(self.encoder_fc.weight, gain=0.01)  # Even smaller gain\n",
    "        nn.init.zeros_(self.encoder_fc.bias)\n",
    "        \n",
    "        # Initialize quantum weights manually\n",
    "        self.q_weights = nn.Parameter(torch.randn(n_layers, n_qubits, 3) * 0.001)  # Very small initialization\n",
    "        \n",
    "        # Readout layer\n",
    "        self.readout = nn.Linear(n_qubits, n_classes)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B, latent_dim)\n",
    "        batch_size = h.shape[0]\n",
    "        \n",
    "        # Check input for NaN\n",
    "        if torch.isnan(h).any() or torch.isinf(h).any():\n",
    "            print(\"⚠️ NaN/Inf in input h!\")\n",
    "            h = torch.nan_to_num(h, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        z = self.encoder_fc(h)  # (B, 2**n_qubits)\n",
    "        \n",
    "        # Aggressive NaN/Inf replacement\n",
    "        z = torch.nan_to_num(z, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # Clamp BEFORE normalization\n",
    "        z = torch.clamp(z, min=-5, max=5)\n",
    "        \n",
    "        # Normalize per row with extreme safety\n",
    "        norm = torch.sqrt(torch.sum(z**2, dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        # If norm is too small, replace entire row with uniform vector\n",
    "        small_norm_mask = norm.squeeze() < 1e-6\n",
    "        if small_norm_mask.any():\n",
    "            print(f\"⚠️ {small_norm_mask.sum().item()} rows with near-zero norm, fixing...\")\n",
    "            uniform_vec = torch.ones(2**self.n_qubits, device=z.device) / np.sqrt(2**self.n_qubits)\n",
    "            z[small_norm_mask] = uniform_vec\n",
    "            norm = torch.sqrt(torch.sum(z**2, dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        z_normalized = z / norm\n",
    "        \n",
    "        # Final verification\n",
    "        z_normalized = torch.nan_to_num(z_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Re-normalize after NaN replacement\n",
    "        final_norm = torch.sqrt(torch.sum(z_normalized**2, dim=1, keepdim=True) + 1e-10)\n",
    "        z_normalized = z_normalized / final_norm\n",
    "        \n",
    "        # Manual batching - process one sample at a time\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            single_input = z_normalized[i].detach().clone()\n",
    "            \n",
    "            # Triple check this specific sample\n",
    "            if torch.isnan(single_input).any() or torch.isinf(single_input).any():\n",
    "                print(f\"⚠️ Sample {i} still has NaN/Inf! Using uniform vector.\")\n",
    "                single_input = torch.ones(2**self.n_qubits, device=z.device) / np.sqrt(2**self.n_qubits)\n",
    "            \n",
    "            # Verify norm is close to 1\n",
    "            sample_norm = torch.sqrt(torch.sum(single_input**2))\n",
    "            if abs(sample_norm - 1.0) > 0.1:\n",
    "                print(f\"⚠️ Sample {i} norm is {sample_norm.item():.4f}, renormalizing...\")\n",
    "                single_input = single_input / (sample_norm + 1e-10)\n",
    "            \n",
    "            try:\n",
    "                # Run quantum circuit on single sample\n",
    "                expvals = qnode(single_input, self.q_weights)\n",
    "                expvals_tensor = torch.stack(expvals).float()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Quantum circuit failed for sample {i}: {e}\")\n",
    "                # Return zeros if circuit fails\n",
    "                expvals_tensor = torch.zeros(self.n_qubits, device=z.device)\n",
    "            \n",
    "            results.append(expvals_tensor)\n",
    "        \n",
    "        # Stack all results back into batch\n",
    "        expvals_batch = torch.stack(results)  # (B, n_qubits)\n",
    "        \n",
    "        # Final readout layer\n",
    "        return self.readout(expvals_batch)\n",
    "\n",
    "print(f\"Quantum device initialized: {dev}\")\n",
    "print(\"QuantumHead with EXTREME safety checks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb7783",
   "metadata": {},
   "source": [
    "## Hybrid Quantum-Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa6eb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model recreated with dtype fix!\n",
      "Parameters: 6,720,647\n"
     ]
    }
   ],
   "source": [
    "model = HybridQML(img_size=img_size, latent_dim=latent_dim, n_classes=14).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-5,\n",
    "    eps=1e-7\n",
    ")\n",
    "\n",
    "clf_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"✓ Model recreated with dtype fix!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051572bf",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a244a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training functions defined successfully!\n",
      "Model has 6720647 parameters\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = HybridQML(img_size=img_size, latent_dim=latent_dim, n_classes=14, recon_weight=0.05).to(device)\n",
    "\n",
    "clf_criterion = nn.BCEWithLogitsLoss()  # multi-label loss\n",
    "recon_criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5, eps=1e-7)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        logits, x_rec = model(imgs, return_recon=True)\n",
    "        loss = clf_criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # GRADIENT CLIPPING - prevents NaN!\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)}, Avg Loss: {total/n:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: train loss = {total/n:.4f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = clf_criterion(logits, labels)\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "    print(f\"Test BCEWithLogits loss = {total/n:.4f}\")\n",
    "\n",
    "print(\"Training functions defined successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28252c5d",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd7e3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "  Batch 0/675, Avg Loss: 0.7156\n",
      "  Batch 200/675, Avg Loss: 0.7093\n",
      "  Batch 400/675, Avg Loss: 0.7052\n",
      "  Batch 600/675, Avg Loss: 0.7011\n",
      "Epoch 1: train loss = 0.6997\n",
      "Test BCEWithLogits loss = 0.6861\n",
      "\n",
      "  Batch 0/675, Avg Loss: 0.6824\n",
      "  Batch 200/675, Avg Loss: 0.6825\n",
      "  Batch 400/675, Avg Loss: 0.6786\n",
      "  Batch 600/675, Avg Loss: 0.6747\n",
      "Epoch 2: train loss = 0.6733\n",
      "Test BCEWithLogits loss = 0.6602\n",
      "\n",
      "  Batch 0/675, Avg Loss: 0.6625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     evaluate()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     19\u001b[39m loss = clf_criterion(logits, labels)\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# GRADIENT CLIPPING - prevents NaN!\u001b[39;00m\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/workflow/interfaces/torch.py:101\u001b[39m, in \u001b[36mpytreeify.<locals>.new_backward\u001b[39m\u001b[34m(ctx, *flat_grad_outputs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_backward\u001b[39m(ctx, *flat_grad_outputs):\n\u001b[32m    100\u001b[39m     grad_outputs = pytree.tree_unflatten(flat_grad_outputs, ctx._out_struct)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     grad_inputs = \u001b[43morig_bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# None corresponds to the diff of out_struct_holder\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) + \u001b[38;5;28mtuple\u001b[39m(grad_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/workflow/interfaces/torch.py:190\u001b[39m, in \u001b[36mExecuteTapes.backward\u001b[39m\u001b[34m(ctx, *dy)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Torch obeys the dL/dz_conj convention instead of the\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# dL/dz convention of PennyLane, autograd and jax. This converts between the formats\u001b[39;00m\n\u001b[32m    189\u001b[39m dy = _recursive_conj(dy)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m vjps = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# split tensor into separate entries\u001b[39;00m\n\u001b[32m    192\u001b[39m unpacked_vjps = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/workflow/jacobian_products.py:331\u001b[39m, in \u001b[36mTransformJacobianProducts.compute_vjp\u001b[39m\u001b[34m(self, tapes, dy)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_vjps(jacs, dy, tapes)\n\u001b[32m    327\u001b[39m vjp_tapes, processing_fn = qml.gradients.batch_vjp(\n\u001b[32m    328\u001b[39m     tapes, dy, \u001b[38;5;28mself\u001b[39m._gradient_transform, gradient_kwargs=\u001b[38;5;28mself\u001b[39m._gradient_kwargs\n\u001b[32m    329\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m vjp_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvjp_tapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(processing_fn(vjp_results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/workflow/run.py:256\u001b[39m, in \u001b[36m_make_inner_execute.<locals>.inner_execute\u001b[39m\u001b[34m(tapes)\u001b[39m\n\u001b[32m    253\u001b[39m transformed_tapes, transform_post_processing = inner_transform(tapes)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_tapes:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     results = \u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_tapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     results = ()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/modifiers/simulator_tracking.py:28\u001b[39m, in \u001b[36m_track_execute.<locals>.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config=DefaultExecutionConfig):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     results = \u001b[43muntracked_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[32m     30\u001b[39m         batch = (circuits,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/modifiers/single_tape_support.py:30\u001b[39m, in \u001b[36m_make_execute.<locals>.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     28\u001b[39m     is_single_circuit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m     circuits = (circuits,)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m results = \u001b[43mbatch_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/default_qubit.py:707\u001b[39m, in \u001b[36mDefaultQubit.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m    697\u001b[39m     warnings.warn(\n\u001b[32m    698\u001b[39m         (\n\u001b[32m    699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mJitting executions with many circuits may have substantial classical overhead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    702\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebugger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprng_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcm_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpostselect_mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostselect_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    723\u001b[39m vanilla_circuits = convert_to_numpy_parameters(circuits)[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m seeds = \u001b[38;5;28mself\u001b[39m._rng.integers(\u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m, size=\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/default_qubit.py:708\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    697\u001b[39m     warnings.warn(\n\u001b[32m    698\u001b[39m         (\n\u001b[32m    699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mJitting executions with many circuits may have substantial classical overhead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    702\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebugger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprng_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcm_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpostselect_mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostselect_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m c, _key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(circuits, prng_keys)\n\u001b[32m    721\u001b[39m     )\n\u001b[32m    723\u001b[39m vanilla_circuits = convert_to_numpy_parameters(circuits)[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m seeds = \u001b[38;5;28mself\u001b[39m._rng.integers(\u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m, size=\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/default_qubit.py:1055\u001b[39m, in \u001b[36m_simulate_wrapper\u001b[39m\u001b[34m(circuit, kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_simulate_wrapper\u001b[39m(circuit, kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/qubit/simulate.py:364\u001b[39m, in \u001b[36msimulate\u001b[39m\u001b[34m(circuit, debugger, state_cache, **execution_kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     state_cache[circuit.hash] = state\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeasure_final_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeas_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexecution_kwargs\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/qubit/simulate.py:257\u001b[39m, in \u001b[36mmeasure_final_state\u001b[39m\u001b[34m(circuit, state, is_state_batched, **execution_kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(circuit.measurements) == \u001b[32m1\u001b[39m:\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m measure(circuit.measurements[\u001b[32m0\u001b[39m], state, is_state_batched=is_state_batched)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasurements\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# finite-shot case\u001b[39;00m\n\u001b[32m    262\u001b[39m rng = default_rng(rng)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/qubit/simulate.py:258\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(circuit.measurements) == \u001b[32m1\u001b[39m:\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m measure(circuit.measurements[\u001b[32m0\u001b[39m], state, is_state_batched=is_state_batched)\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         \u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m mp \u001b[38;5;129;01min\u001b[39;00m circuit.measurements\n\u001b[32m    259\u001b[39m     )\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# finite-shot case\u001b[39;00m\n\u001b[32m    262\u001b[39m rng = default_rng(rng)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/qubit/measure.py:238\u001b[39m, in \u001b[36mmeasure\u001b[39m\u001b[34m(measurementprocess, state, is_state_batched)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeasure\u001b[39m(\n\u001b[32m    226\u001b[39m     measurementprocess: MeasurementProcess, state: TensorLike, is_state_batched: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    227\u001b[39m ) -> TensorLike:\n\u001b[32m    228\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply a measurement process to a state.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m    230\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m \u001b[33;03m        Tensorlike: the result of the measurement\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_measurement_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurementprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeasurementprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/devices/qubit/measure.py:72\u001b[39m, in \u001b[36mstate_diagonalizing_gates\u001b[39m\u001b[34m(measurementprocess, state, is_state_batched)\u001b[39m\n\u001b[32m     70\u001b[39m wires = Wires(\u001b[38;5;28mrange\u001b[39m(total_indices))\n\u001b[32m     71\u001b[39m flattened_state = flatten_state(state, total_indices)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeasurementprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwires\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/measurements/expval.py:138\u001b[39m, in \u001b[36mExpectationMP.process_state\u001b[39m\u001b[34m(self, state, wire_order)\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m qml.math.squeeze(\u001b[38;5;28mself\u001b[39m.eigvals())\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m qml.queuing.QueuingManager.stop_recording():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     prob = \u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwires\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwires\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwire_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwire_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# In case of broadcasting, `prob` has two axes and this is a matrix-vector product\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._calculate_expectation(prob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/measurements/probs.py:213\u001b[39m, in \u001b[36mProbabilityMP.process_state\u001b[39m\u001b[34m(self, state, wire_order)\u001b[39m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prob\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# determine which subsystems are to be summed over\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m inactive_wires = \u001b[43mWires\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_wires\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwire_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwires\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# translate to wire labels used by device\u001b[39;00m\n\u001b[32m    216\u001b[39m wire_map = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(wire_order, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(wire_order))))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pennylane/wires.py:514\u001b[39m, in \u001b[36mWires.unique_wires\u001b[39m\u001b[34m(list_of_wires)\u001b[39m\n\u001b[32m    512\u001b[39m     seen_once = (seen_once ^ labels) - (seen_ever - seen_once)\n\u001b[32m    513\u001b[39m     \u001b[38;5;66;03m# Update seen labels with all new seen labels\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[43mseen_ever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# Get unique values in order they appear.\u001b[39;00m\n\u001b[32m    517\u001b[39m unique = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate()\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
