{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6e1489",
   "metadata": {},
   "source": [
    "# Quantum-Classical Hybrid Autoencoder for Medical Image Classification\n",
    "\n",
    "This notebook implements a hybrid quantum-classical model for chest X-ray classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "## Installation (Run if packages missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install packages\n",
    "# %pip install torch torchvision numpy pennylane medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89333376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PennyLane version: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a80afb",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 6                 # set desired number of qubits\n",
    "n_layers = 2                 # set desired number of quantum layers\n",
    "latent_dim = 2**n_qubits     # latent dimension = 2^n_qubits\n",
    "img_size = 224               # set desired image size (28, 64, 128, 224)\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Qubits: {n_qubits}\")\n",
    "print(f\"  Quantum layers: {n_layers}\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Image size: {img_size}x{img_size}\")\n",
    "print(f\"  Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74359e0",
   "metadata": {},
   "source": [
    "## Load Data from preprocess.py\n",
    "\n",
    "We import the data loaders from our preprocess.py file to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data loaders from preprocess.py\n",
    "from preprocess import train_loader, test_loader, train_dataset, test_dataset\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd582d",
   "metadata": {},
   "source": [
    "Filter Out Double-Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "print(\"Filtering for single-label images only...\")\n",
    "\n",
    "# Filter training set\n",
    "train_single_label_indices = []\n",
    "for i in range(len(train_dataset)):\n",
    "    _, label = train_dataset[i]\n",
    "    if label.sum() == 1:  # Only one condition present\n",
    "        train_single_label_indices.append(i)\n",
    "\n",
    "train_dataset = Subset(train_dataset, train_single_label_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Filter test set\n",
    "test_single_label_indices = []\n",
    "for i in range(len(test_dataset)):\n",
    "    _, label = test_dataset[i]\n",
    "    if label.sum() == 1:\n",
    "        test_single_label_indices.append(i)\n",
    "\n",
    "test_dataset = Subset(test_dataset, test_single_label_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"✓ Filtered Training: {len(train_dataset)} single-label images ({len(train_loader)} batches)\")\n",
    "print(f\"✓ Filtered Test: {len(test_dataset)} single-label images ({len(test_loader)} batches)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf487ff",
   "metadata": {},
   "source": [
    "## Classical Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64, img_size=224):\n",
    "        super().__init__()\n",
    "        self.h8 = img_size // 8\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize encoder weights properly\n",
    "        for m in self.encoder.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # (B,latent_dim)\n",
    "        return z  # Return unnormalized z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4a989",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- quantum device ----\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "def qnode(inputs, weights):\n",
    "    # inputs: shape (2**n_qubits,), NOT batched - single sample\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    def __init__(self, n_layers, n_qubits, n_classes, latent_dim):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.encoder_fc = nn.Linear(latent_dim, 2**n_qubits)\n",
    "        \n",
    "        # Better weight initialization\n",
    "        nn.init.xavier_uniform_(self.encoder_fc.weight, gain=0.01)  # Even smaller gain\n",
    "        nn.init.zeros_(self.encoder_fc.bias)\n",
    "        \n",
    "        # Initialize quantum weights manually\n",
    "        self.q_weights = nn.Parameter(torch.randn(n_layers, n_qubits, 3) * 0.001)  # Very small initialization\n",
    "        \n",
    "        # Readout layer\n",
    "        self.readout = nn.Linear(n_qubits, n_classes)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B, latent_dim)\n",
    "        batch_size = h.shape[0]\n",
    "        \n",
    "        # Check input for NaN\n",
    "        if torch.isnan(h).any() or torch.isinf(h).any():\n",
    "            print(\"⚠️ NaN/Inf in input h!\")\n",
    "            h = torch.nan_to_num(h, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        z = self.encoder_fc(h)  # (B, 2**n_qubits)\n",
    "        \n",
    "        # Aggressive NaN/Inf replacement\n",
    "        z = torch.nan_to_num(z, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # Clamp BEFORE normalization\n",
    "        z = torch.clamp(z, min=-5, max=5)\n",
    "        \n",
    "        # Normalize per row with extreme safety\n",
    "        norm = torch.sqrt(torch.sum(z**2, dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        # If norm is too small, replace entire row with uniform vector\n",
    "        small_norm_mask = norm.squeeze() < 1e-6\n",
    "        if small_norm_mask.any():\n",
    "            print(f\"⚠️ {small_norm_mask.sum().item()} rows with near-zero norm, fixing...\")\n",
    "            uniform_vec = torch.ones(2**self.n_qubits, device=z.device) / np.sqrt(2**self.n_qubits)\n",
    "            z[small_norm_mask] = uniform_vec\n",
    "            norm = torch.sqrt(torch.sum(z**2, dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        z_normalized = z / norm\n",
    "        \n",
    "        # Final verification\n",
    "        z_normalized = torch.nan_to_num(z_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Re-normalize after NaN replacement\n",
    "        final_norm = torch.sqrt(torch.sum(z_normalized**2, dim=1, keepdim=True) + 1e-10)\n",
    "        z_normalized = z_normalized / final_norm\n",
    "        \n",
    "        # Manual batching - process one sample at a time\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            single_input = z_normalized[i].detach().clone()\n",
    "            \n",
    "            # Triple check this specific sample\n",
    "            if torch.isnan(single_input).any() or torch.isinf(single_input).any():\n",
    "                print(f\"⚠️ Sample {i} still has NaN/Inf! Using uniform vector.\")\n",
    "                single_input = torch.ones(2**self.n_qubits, device=z.device) / np.sqrt(2**self.n_qubits)\n",
    "            \n",
    "            # Verify norm is close to 1\n",
    "            sample_norm = torch.sqrt(torch.sum(single_input**2))\n",
    "            if abs(sample_norm - 1.0) > 0.1:\n",
    "                print(f\"⚠️ Sample {i} norm is {sample_norm.item():.4f}, renormalizing...\")\n",
    "                single_input = single_input / (sample_norm + 1e-10)\n",
    "            \n",
    "            try:\n",
    "                # Run quantum circuit on single sample\n",
    "                expvals = qnode(single_input, self.q_weights)\n",
    "                expvals_tensor = torch.stack(expvals).float()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Quantum circuit failed for sample {i}: {e}\")\n",
    "                # Return zeros if circuit fails\n",
    "                expvals_tensor = torch.zeros(self.n_qubits, device=z.device)\n",
    "            \n",
    "            results.append(expvals_tensor)\n",
    "        \n",
    "        # Stack all results back into batch\n",
    "        expvals_batch = torch.stack(results)  # (B, n_qubits)\n",
    "        \n",
    "        # Final readout layer\n",
    "        return self.readout(expvals_batch)\n",
    "\n",
    "print(f\"Quantum device initialized: {dev}\")\n",
    "print(\"QuantumHead with EXTREME safety checks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb7783",
   "metadata": {},
   "source": [
    "## Hybrid Quantum-Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6eb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridQML(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim, n_classes=14):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(latent_dim=latent_dim, img_size=img_size)\n",
    "        self.qhead = QuantumHead(n_layers=n_layers, n_qubits=n_qubits, n_classes=n_classes, latent_dim=latent_dim)\n",
    "\n",
    "    def forward(self, x, return_recon=False):\n",
    "        h = self.enc(x)\n",
    "        out = self.qhead(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051572bf",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = HybridQML(img_size=img_size, latent_dim=latent_dim, n_classes=14).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-5,\n",
    "    eps=1e-7\n",
    ")\n",
    "\n",
    "clf_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"✓ Model recreated with dtype fix!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    window_start = time.time()\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        logits = model(imgs, return_recon=True)\n",
    "        loss = clf_criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # GRADIENT CLIPPING - prevents NaN!\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "        \n",
    "\n",
    "        avg_size = 10\n",
    "        if batch_idx % avg_size == 0:  \n",
    "            # measure total time for the last 10 batches\n",
    "            elapsed = time.time() - window_start\n",
    "            avg_time = elapsed / avg_size if batch_idx != 0 else elapsed\n",
    "            print(\n",
    "                f\"  Batch {batch_idx+1}/{len(train_loader)}, \"\n",
    "                f\"Avg Loss: {total/n:.4f}, \"\n",
    "                f\"Avg Time per batch: {avg_time:.4f}s\"\n",
    "            )\n",
    "            window_start = time.time()  # reset window\n",
    "            \n",
    "\n",
    "    print(f\"Epoch {epoch}: train loss = {total/n:.4f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = clf_criterion(logits, labels)\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "    print(f\"Test BCEWithLogits loss = {total/n:.4f}\")\n",
    "\n",
    "print(\"Training functions defined successfully!\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28252c5d",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate()\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ba419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"hybrid_qml_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
